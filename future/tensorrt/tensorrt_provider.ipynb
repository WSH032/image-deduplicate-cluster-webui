{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\GitHub\\image-deduplicate-cluster-webui\\venv\\Scripts;E:\\GitHub\\image-deduplicate-cluster-webui\\venv\\Scripts;E:\\VMware\\VMware Player\\bin\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;E:\\Program Files\\MATLAB\\R2022a\\runtime\\win64;E:\\Program Files\\MATLAB\\R2022a\\bin;C:\\Program Files (x86)\\NoteBook FanControl\\;C:\\Program Files\\dotnet\\;D:\\Git\\cmd;C:\\MinGW\\bin;C:\\Users\\WSH\\AppData\\Local\\Programs\\Python\\Python310\\Scripts\\;C:\\Users\\WSH\\AppData\\Local\\Programs\\Python\\Python310\\;D:\\Cadence_SPB_16.6\\Cadence\\Cadence_SPB_16.6\\openaccess\\bin\\win32\\opt;D:\\Cadence_SPB_16.6\\Cadence\\Cadence_SPB_16.6\\tools\\capture;D:\\Cadence_SPB_16.6\\Cadence\\Cadence_SPB_16.6\\tools\\pspice;D:\\Cadence_SPB_16.6\\Cadence\\Cadence_SPB_16.6\\tools\\specctra\\bin;D:\\Cadence_SPB_16.6\\Cadence\\Cadence_SPB_16.6\\tools\\fet\\bin;D:\\Cadence_SPB_16.6\\Cadence\\Cadence_SPB_16.6\\tools\\libutil\\bin;D:\\Cadence_SPB_16.6\\Cadence\\Cadence_SPB_16.6\\tools\\bin;D:\\Cadence_SPB_16.6\\Cadence\\Cadence_SPB_16.6\\tools\\pcb\\bin;C:\\Users\\WSH\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\WSH\\AppData\\Local\\GitHubDesktop\\bin;E:\\Microsoft VS Code\\bin;E:\\Microsoft VS Code Insiders\\bin;E:\\GitHub\\image-deduplicate-cluster-webui\\venv\\Lib\\site-packages\\torch\\lib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 添加环境变量\n",
    "new_path = r'E:\\GitHub\\image-deduplicate-cluster-webui\\venv\\Lib\\site-packages\\torch\\lib'\n",
    "os.environ['PATH'] += os.pathsep + new_path\n",
    "\n",
    "# 打印环境变量\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT version: 8.6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "print(\"TensorRT version: {}\".format(trt.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime version: 1.15.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# 打印 ONNX Runtime 版本信息\n",
    "print(\"ONNX Runtime version:\", ort.__version__)\n",
    "\n",
    "# 检查是否可用 GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is NOT available\")\n",
    "\n",
    "import os\n",
    "model_path = r\"E:\\GitHub\\image-deduplicate-cluster-webui\\wd14_tagger_model\\model.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your cache files will be stored in E:\\GitHub\\image-deduplicate-cluster-webui\\wd14_tagger_model\\trt_engine_cache\n",
      "Warning: Please clean up any old engine and profile cache files (.engine and .profile) if any of the following changes:\n",
      "1.Model changes (if there are any changes to the model topology, opset version, operators etc.)\n",
      "2.ORT version changes (i.e. moving from ORT version 1.8 to 1.9)\n",
      "3.TensorRT version changes (i.e. moving from TensorRT 7.0 to 8.0)\n",
      "4.Hardware changes. (Engine and profile files are not portable and optimized for specific Nvidia hardware)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 缓存在与onnx模型同目录下的'trt_engine_cache'文件夹中\n",
    "trt_engine_cache_path = os.path.join( os.path.dirname(model_path), \"trt_engine_cache\" )\n",
    "Tensorrt_options = {\"trt_timing_cache_enable\": True,  # 时序缓存,可以适用于多个模型\n",
    "                    \"trt_engine_cache_enable\": True,  # 开启引擎缓存,针对特定模型、推理参数、GPU\n",
    "                    \"trt_engine_cache_path\":trt_engine_cache_path,\n",
    "                    \"trt_fp16_enable\": False,  # FP16模式，需要GPU支持\n",
    "                    \"trt_int8_enable\": False,  # INT8模式，需要GPU支持\n",
    "                    \"trt_dla_enable\": False,  # DLA深度学习加速器，需要GPU支持\n",
    "                    \"trt_build_heuristics_enable\" : False,  # 启用启发式构建，减少时间\n",
    "                    \"trt_builder_optimization_level\": 3,  # 优化等级，越小耗时越少，但优化更差，不建议低于3\n",
    "}\n",
    "Tensorrt_provider = (\"TensorrtExecutionProvider\", Tensorrt_options)\n",
    "\n",
    "if Tensorrt_options[\"trt_engine_cache_enable\"]:\n",
    "    print(f\"\"\"\n",
    "Your cache files will be stored in {Tensorrt_options[\"trt_engine_cache_path\"]}\n",
    "Warning: Please clean up any old engine and profile cache files (.engine and .profile) if any of the following changes:\n",
    "1.Model changes (if there are any changes to the model topology, opset version, operators etc.)\n",
    "2.ORT version changes (i.e. moving from ORT version 1.8 to 1.9)\n",
    "3.TensorRT version changes (i.e. moving from TensorRT 7.0 to 8.0)\n",
    "4.Hardware changes. (Engine and profile files are not portable and optimized for specific Nvidia hardware)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用执行者 ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "print(\"注册的执行者\", ort.get_available_providers())\n",
    "\n",
    "use_cuda = True\n",
    "use_tensorrt = True\n",
    "\n",
    "EP_list = ['CPUExecutionProvider']\n",
    "if use_cuda:\n",
    "    EP_list += ['CUDAExecutionProvider']\n",
    "if use_tensorrt:\n",
    "    EP_list += [Tensorrt_provider]\n",
    "\n",
    "print(\"最终选择的执行者\", EP_list)\n",
    "\n",
    "# initialize the model.onnx\n",
    "sess = ort.InferenceSession(model_path, providers=[Tensorrt_provider])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost time: 21.170454740524292\n",
      "[array([[7.8934796e-02, 5.6522924e-01, 2.3527807e-01, ..., 6.0830089e-06,\n",
      "        3.2220836e-05, 4.7021302e-05]], dtype=float32)]\n",
      "(1, 9083)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "# 获取输出\n",
    "start = time.time()\n",
    "\n",
    "# get the outputs metadata as a list of :class:`onnxruntime.NodeArg`\n",
    "output_name = sess.get_outputs()[0].name\n",
    "for i in range(360):\n",
    "    input_arr = np.random.randint(0, 225, size=(1, 448, 448, 3)).astype(np.float32)\n",
    "    ort_inputs = {sess.get_inputs()[0].name: input_arr,}\n",
    "    ort_outs = sess.run(None, ort_inputs)\n",
    "print(\"cost time:\", time.time() - start)\n",
    "print(ort_outs)\n",
    "print(ort_outs[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
